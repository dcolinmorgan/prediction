{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LTRC_TS_LSTM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfQQBfpYsRbckrmI4hAVWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcolinmorgan/test/blob/master/LTRC_TS_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6D4OBoWTspS"
      },
      "source": [
        "# LTCOPD dataset analyzed for time series prediction\n",
        "\n",
        "ran panda nets for control and case [HERE in \"run large PANDA\"](https://colab.research.google.com/drive/1ER3FudwHf62xpVaqtA55XevX--E4INUe?authuser=1#scrollTo=3NiZai_nLIts&uniqifier=2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBC7nKMZTqWw"
      },
      "source": [
        "import os,glob,gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from psutil import *\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from scipy.stats import zscore\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoBt11zUAI-V"
      },
      "source": [
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, LSTM, Conv1D, Bidirectional, Lambda,Dropout"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70weiRHHTryg",
        "outputId": "c198ba56-6bb9-4582-8065-91f069e123d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVorEUSGWjcn"
      },
      "source": [
        "links=pd.read_csv('drive/My Drive/LARGE_PANDA/LARGECOPD_links.txt',names=['TF','gene'])\n",
        "CASE=np.load('drive/My Drive/LARGE_PANDA/LTRCcase.npy')\n",
        "CASE_ages=pd.read_csv('drive/My Drive/LARGE_PANDA/LTRCcase_ages.txt',names=['ages'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AefO9Og4A42o"
      },
      "source": [
        "CASE=pd.DataFrame(CASE)\n",
        "CASE.columns=CASE_ages[0:]['ages']\n",
        "# CASE.index=links.TF+'_'+links.gene"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVYoT4NWxBFL"
      },
      "source": [
        "# links=pd.read_csv('drive/My Drive/LARGE_PANDA/LARGECOPD_links.txt',names=['TF','gene'])\n",
        "# CONTROL=np.load('drive/My Drive/LARGE_PANDA/LTRCcontrol.npy')\n",
        "# CONTROL_ages=pd.read_csv('drive/My Drive/LARGE_PANDA/LTRCcontrol_ages.txt',names=['ages'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX_-8dww725C"
      },
      "source": [
        "# CONTROL=pd.DataFrame(CONTROL)\n",
        "# CONTROL.columns=CONTROL_ages[0:]['ages']\n",
        "# CONTROL.index=links.TF+'_'+links.gene"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybt2jwbMAAy7"
      },
      "source": [
        "# FORMAT DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKNQT2rdBhtl"
      },
      "source": [
        "## CASE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOGbOreYATHC"
      },
      "source": [
        "tdata=pd.DataFrame(CASE.T, index=CASE.columns)#.interpolate(method='time')\n",
        "tdata['Datetime'] = pd.to_datetime('19'+(CASE.columns.astype(str)) + ' ' +'10:00:00 ')\n",
        "del CASE\n",
        "CASE=pd.DataFrame()\n",
        "tdata = tdata.set_index('Datetime')\n",
        "# ALL.iloc[1,:].to_list()#.dropna(how='any')\n",
        "tdata=tdata.interpolate(method='time')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBWOolHABjtN"
      },
      "source": [
        "##CONTROL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUWL0xf_Bj1L"
      },
      "source": [
        "# tdata=pd.DataFrame(CASE.T, index=CASE.columns)#.interpolate(method='time')\n",
        "# tdata['Datetime'] = pd.to_datetime('19'+(CASE.columns.astype(str)) + ' ' +'10:00:00 ')\n",
        "# tdata = tdata.set_index('Datetime')\n",
        "# # ALL.iloc[1,:].to_list()#.dropna(how='any')\n",
        "# tdata=tdata.interpolate(method='time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz45hB_9ABuh"
      },
      "source": [
        "split_time = 30\n",
        "heat=pd.DataFrame()\n",
        "window_size = 6\n",
        "batch_size = 10\n",
        "shuffle_buffer_size = 10\n",
        "\n",
        "i=int(np.random.randint(1, high=ttdata.shape[0], size=1) )\n",
        "series=np.array(ttdata.iloc[i,:])\n",
        "\n",
        "time_train = ALL.columns[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = ALL.columns[split_time:]\n",
        "x_valid = series[split_time:]\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLcPXZ1rBFiZ"
      },
      "source": [
        "dataset_x = np.array(tdata)#np.concatenate((dataset_delta_DJI, dataset_delta_APPL, dataset_delta_AMAZN), axis = 1)\n",
        "# dataset_y = dataset_delta_DJI\n",
        "# normalize the dataset\n",
        "scaler_multi = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_multi.fit_transform(dataset_x.reshape(-1, 1))\n",
        "dataset_x = scaler_multi.transform(dataset_x)\n",
        "# dataset_y = scaler_multi.transform(dataset_y)\n",
        "# split into train and test sets\n",
        "# dataset_x\n",
        "train_size = int(len(dataset_x) * 0.67)\n",
        "test_size = len(dataset_x) - train_size\n",
        "train_x, test_x = dataset_x[0:train_size,:], dataset_x[train_size:len(dataset_x),:]\n",
        "\n",
        "# train_x=train_x.reshape(1,train_x.shape[0],train_x.shape[1])\n",
        "\n",
        "# train_y, test_y = dataset_y[0:train_size,:], dataset_y[train_size:len(dataset_y),:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fozi27hKBFlC"
      },
      "source": [
        "# dataset_xx=pd.DataFrame(dataset_x)\n",
        "# dataset_yy=pd.DataFrame(0,index=tdata.index[0:5],columns=tdata.columns)\n",
        "# dataset_xx.append(dataset_yy)\n",
        "n_input=6\n",
        "tdata=tdata[::-1]\n",
        "add_dates = [tdata.index[-1] + DateOffset(years=x) for x in range(0,n_input+1) ]\n",
        "future_dates = pd.DataFrame(0,index=add_dates[1:],columns=tdata.columns)\n",
        "\n",
        "t2data=tdata.append(future_dates)\n",
        "tdata=tdata[::-1]\n",
        "t2data=t2data[::-1]\n",
        "t2data=np.array(t2data)\n",
        "\n",
        "scaler_multi = MinMaxScaler(feature_range=(0, 1)) ## batch normalization between layers\n",
        "scaler_multi.fit_transform(t2data.reshape(-1, 1))\n",
        "t2data = scaler_multi.transform(t2data)\n",
        "\n",
        "# tdata['Datetime'] = pd.to_datetime('19'+(ALL.columns.astype(str)) + ' ' +'10:00:00 ')\n",
        "# tdata = tdata.set_index('Datetime')\n",
        "# # ALL.iloc[1,:].to_list()#.dropna(how='any')\n",
        "# tdata=tdata.interpolate(method='time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9vVwEkkBIac"
      },
      "source": [
        "# look_back = 6\n",
        "# train_data_gen = TimeseriesGenerator(train_x, train_x,\n",
        "#                                length=look_back, sampling_rate=1,stride=1,\n",
        "#                                batch_size=6)\n",
        "# test_data_gen = TimeseriesGenerator(test_x, test_x,\n",
        "#                                length=look_back, sampling_rate=1,stride=1,\n",
        "#                                batch_size=1)\n",
        "\n",
        "# pred_data_gen = TimeseriesGenerator(t2data[::-1], t2data[::-1],\n",
        "#                                length=look_back, sampling_rate=1,stride=1,\n",
        "#                                batch_size=1)\n",
        "\n",
        "\n",
        "train = t2data[::-1]\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "n_input = 5\n",
        "lookback=n_input\n",
        "n_features = t2data.data.shape[1]\n",
        "generator = TimeseriesGenerator(train,train,sampling_rate=1, length=lookback)#, batch_size=6)\n",
        "\n",
        "\n",
        "train_x=train[:-12]\n",
        "train_y=train[-12:-6]\n",
        "  ### subset\n",
        "\n",
        "trainA=train[:-6] \n",
        "# trainA=trainA[:,np.var(trainA,axis=0)<.05]\n",
        "trainA=trainA[:,np.var(trainA,axis=0)<.0001]\n",
        "train_x=trainA[:-6]\n",
        "train_y=trainA[-6:]\n",
        "\n",
        "training_sequence = TimeseriesGenerator(train_x, train_x, length=lookback,batch_size=1)\n",
        "validation_sequence = TimeseriesGenerator(train_y, train_y,  length=lookback,batch_size=1)\n",
        "\n",
        "# look_back = 1 \n",
        "# train_x, train_y = create_data_set(train, look_back) \n",
        "# # test_x, test_y = create_data_set(test, look_back) \n",
        "# train_x = np.reshape(train_x,\n",
        "#                         (train_x.shape[0], 1, train_x.shape[1])) \n",
        "# # test_x = np.reshape(test_x, \n",
        "# #                        (test_x.shape[0], 1, test_x.shape[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erxusk9BKnm"
      },
      "source": [
        "model = Sequential()\n",
        "## graph neural network then (lower D space) to LSTM (neighborhood) NOT conv1d (spatial from images)\n",
        "initializer=tf.keras.initializers.Orthogonal()\n",
        "model.add(LSTM(256, input_shape=(lookback, training_sequence.data.shape[1]),return_sequences=True, kernel_initializer=initializer)) #orthoganal initialization for weight, he  # 5 time steps and 32831 features/genes\n",
        "model.add(Bidirectional(LSTM(128,activation='relu',return_sequences=True))),\n",
        "model.add((LSTM(128,activation='relu',return_sequences=True))),\n",
        "model.add((LSTM(64,activation='relu',return_sequences=True))),\n",
        "model.add((LSTM(48,activation='relu',return_sequences=True))),\n",
        "model.add((LSTM(32,activation='relu'))),\n",
        "# model.add(Dropout(0.15))\n",
        "# model.add((LSTM(32,activation='relu'))),\n",
        "model.add(Dense(training_sequence.data.shape[1]))\n",
        "model.compile(loss='mse', optimizer='adam',metrics=['mse','accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# generator = TimeseriesGenerator(dataset, dataset, length=n_input, batch_size=1)\n",
        "# # define model\n",
        "# model = Sequential()\n",
        "# model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))   # 2 x 3\n",
        "# model.add(Dense(3))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        "# # fit model\n",
        "# model.fit_generator(generator, steps_per_epoch=1, epochs=500, verbose=0)\n",
        "# # make a one step prediction out of sample\n",
        "# x_input = array([[90, 95], [100, 105],[12,21]]).reshape((1, n_input, n_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJEckTOjBNlR"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "from IPython.display import Image\n",
        "Image(filename='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGzevcHWBOku"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, LearningRateScheduler,ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 20)),\n",
        "    EarlyStopping(patience=5, verbose=1),#monitor='loss'),\n",
        "    # ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
        "    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9O9Liy3BQJe"
      },
      "source": [
        "history =model.fit(training_sequence, epochs=100, callbacks=callbacks,validation_data=validation_sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJO3kn4MBQh6"
      },
      "source": [
        "model.save('drive/MyDrive/LARGE_PANDA/model_output/CASE.h')\n",
        "# model.save('drive/MyDrive/LARGE_PANDA/model_output/CONTROL.h')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae4nPf5tBQkJ"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18, 8))\n",
        "plt.plot(hist['epoch'],hist['val_loss'], color= 'green')\n",
        "# plt.plot(loss, color='red')\n",
        "# plt.title(\"Close price of stocks sold\")\n",
        "plt.xlabel(\"EPOCH\")\n",
        "plt.ylabel(\"VAL_LOSS\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8YwcNRWCLkX"
      },
      "source": [
        "# load and predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFWopziEBQmy"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('drive/MyDrive/LARGE_PANDA/model_output/CASE.h')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRjDy_wNBwm3"
      },
      "source": [
        "t2data=t2data[::-1]\n",
        "train = t2data[:-30]\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "# batch=[]\n",
        "pred_list = []\n",
        "# batch = train_y[-n_input:].reshape((1, n_input, t2data.shape[1]))\n",
        "\n",
        "##subset\n",
        "train = trainA\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "train = scaler.transform(train)\n",
        "batch = train_y[-n_input:].reshape((1, n_input, trainA.shape[1]))\n",
        "\n",
        "# batch=batch.T\n",
        "for i in range(n_input):\n",
        "  # print(batch[0,1:,:])\n",
        "  pred_list.append(model.predict(batch))#[0])  ## comment T then uncomment\n",
        "  batch = np.append(batch[0,1:,:],pred_list[i],axis=0)\n",
        "  # print([batch.shape,pred_list[i].T.shape])\n",
        "  # batch=batch.T\n",
        "  # print(pred_list[i])\n",
        "  batch = np.expand_dims(batch, 0)\n",
        "  # print([i,batch.shape])\n",
        "print(pd.DataFrame(np.squeeze(pred_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ogOHOdEBwpf"
      },
      "source": [
        "# pred_list=pd.DataFrame(np.reshape(pred_list,[lookback,t2data.shape[1]]))\n",
        "pred_list=pd.DataFrame(np.reshape(pred_list,[lookback,trainA.shape[1]]))\n",
        "# pred_list=scaler.inverse_transform((pred_list))\n",
        "# scaler = MinMaxScaler()\n",
        "# scaler.fit(trainA)\n",
        "# pred_list=scaler.transform((pred_list))\n",
        "# pred_list = []\n",
        "# batch = train[-n_input:].reshape((1, n_input, t2data.shape[1]))\n",
        "# pred_list.append(model.predict(batch.T))\n",
        "pred_list=(np.squeeze(pred_list))\n",
        "pred_list=pd.DataFrame(pred_list)\n",
        "print(pred_list)\n",
        "\n",
        "# pred=pd.DataFrame(t2data[::-1])[6:]\n",
        "pred=pd.DataFrame(trainA)\n",
        "pred=pred.append(pred_list)\n",
        "pred=pred.reset_index()\n",
        "del pred['index']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQawRE96BzDW"
      },
      "source": [
        "w=plt.plot(pred.iloc[0:42,0:9])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}